{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdf50cd-da7c-4d60-994f-33d6797a9f5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d474e1-e671-4cb6-88e0-cc0964fbaefb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53ed11a-e92f-4d83-a830-f9e4eb2da4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Trying config: TD3Config(actor_lr=0.0001, critic_lr=0.001, gamma=0.99, tau=0.005, batch_size=64, policy_noise_frac=0.1, noise_clip_frac=0.2, explore_noise_frac=0.05, policy_delay=2, max_episodes=400, device='cpu', seed=0) ===\n",
      "[TD3] Episode   25 | Reward: -1.2712 | Final DLS:  134.90 nm\n",
      "[TD3] Episode   50 | Reward: -1.2583 | Final DLS:  134.90 nm\n",
      "[TD3] Episode   75 | Reward: -1.2583 | Final DLS:  134.90 nm\n",
      "[TD3] Episode  100 | Reward: -1.2893 | Final DLS:  134.28 nm\n",
      "[TD3] Episode  125 | Reward: -1.2634 | Final DLS:  134.90 nm\n",
      "[TD3] Episode  150 | Reward: -1.2660 | Final DLS:  134.90 nm\n",
      "[TD3] Episode  175 | Reward: -1.2583 | Final DLS:  134.90 nm\n",
      "[TD3] Episode  200 | Reward: -1.2634 | Final DLS:  134.90 nm\n",
      "[TD3] Episode  225 | Reward: -1.2583 | Final DLS:  134.90 nm\n",
      "[TD3] Episode  250 | Reward: -1.2686 | Final DLS:  134.90 nm\n",
      "[TD3] Episode  275 | Reward: -1.2634 | Final DLS:  134.90 nm\n",
      "[TD3] Episode  300 | Reward: -1.2583 | Final DLS:  134.90 nm\n",
      "Avg reward (last 50): -1.2643\n",
      "\n",
      "=== Trying config: TD3Config(actor_lr=0.0003, critic_lr=0.0003, gamma=0.99, tau=0.005, batch_size=64, policy_noise_frac=0.1, noise_clip_frac=0.2, explore_noise_frac=0.1, policy_delay=2, max_episodes=400, device='cpu', seed=0) ===\n",
      "[TD3] Episode   25 | Reward: -1.4898 | Final DLS:  134.90 nm\n",
      "[TD3] Episode   50 | Reward: -1.2712 | Final DLS:  134.90 nm\n",
      "[TD3] Episode   75 | Reward: -1.2789 | Final DLS:  134.90 nm\n",
      "[TD3] Episode  100 | Reward: -1.5588 | Final DLS:  133.04 nm\n",
      "[TD3] Episode  125 | Reward: -1.2789 | Final DLS:  134.90 nm\n",
      "[TD3] Episode  150 | Reward: -1.3012 | Final DLS:  134.90 nm\n",
      "[TD3] Episode  175 | Reward: -1.2583 | Final DLS:  134.90 nm\n",
      "[TD3] Episode  200 | Reward: -1.2919 | Final DLS:  134.28 nm\n",
      "[TD3] Episode  225 | Reward: -1.3022 | Final DLS:  134.90 nm\n",
      "[TD3] Episode  250 | Reward: -1.2945 | Final DLS:  134.90 nm\n",
      "[TD3] Episode  275 | Reward: -1.2867 | Final DLS:  134.90 nm\n",
      "[TD3] Episode  300 | Reward: -1.2634 | Final DLS:  134.90 nm\n",
      "Avg reward (last 50): -1.3029\n",
      "\n",
      "=== Trying config: TD3Config(actor_lr=0.0001, critic_lr=0.001, gamma=0.99, tau=0.005, batch_size=64, policy_noise_frac=0.15, noise_clip_frac=0.25, explore_noise_frac=0.08, policy_delay=2, max_episodes=400, device='cpu', seed=0) ===\n",
      "[TD3] Episode   25 | Reward: -1.2909 | Final DLS:  134.90 nm\n",
      "[TD3] Episode   50 | Reward: -1.2634 | Final DLS:  134.90 nm\n",
      "[TD3] Episode   75 | Reward: -1.2686 | Final DLS:  134.90 nm\n",
      "[TD3] Episode  100 | Reward: -1.3048 | Final DLS:  134.28 nm\n",
      "[TD3] Episode  125 | Reward: -1.2738 | Final DLS:  134.90 nm\n",
      "[TD3] Episode  150 | Reward: -1.2961 | Final DLS:  134.90 nm\n",
      "[TD3] Episode  175 | Reward: -1.2583 | Final DLS:  134.90 nm\n",
      "[TD3] Episode  200 | Reward: -1.2738 | Final DLS:  134.28 nm\n",
      "[TD3] Episode  225 | Reward: -1.2841 | Final DLS:  134.90 nm\n",
      "[TD3] Episode  250 | Reward: -1.2789 | Final DLS:  134.90 nm\n",
      "[TD3] Episode  275 | Reward: -1.2764 | Final DLS:  134.90 nm\n",
      "[TD3] Episode  300 | Reward: -1.2634 | Final DLS:  134.90 nm\n",
      "Avg reward (last 50): -1.2797\n",
      "\n",
      "=== Trying config: TD3Config(actor_lr=0.0001, critic_lr=0.001, gamma=0.99, tau=0.005, batch_size=64, policy_noise_frac=0.1, noise_clip_frac=0.2, explore_noise_frac=0.05, policy_delay=2, max_episodes=400, device='cpu', seed=1) ===\n",
      "[TD3] Episode   25 | Reward: -2.4285 | Final DLS:  120.86 nm\n",
      "[TD3] Episode   50 | Reward: -2.4285 | Final DLS:  120.86 nm\n",
      "[TD3] Episode   75 | Reward: -2.4285 | Final DLS:  120.86 nm\n",
      "[TD3] Episode  100 | Reward: -2.4285 | Final DLS:  120.86 nm\n",
      "[TD3] Episode  125 | Reward: -2.4285 | Final DLS:  120.86 nm\n",
      "[TD3] Episode  150 | Reward: -2.4285 | Final DLS:  120.86 nm\n",
      "[TD3] Episode  175 | Reward: -2.4285 | Final DLS:  120.86 nm\n",
      "[TD3] Episode  200 | Reward: -2.4285 | Final DLS:  120.86 nm\n",
      "[TD3] Episode  225 | Reward: -2.4285 | Final DLS:  120.86 nm\n",
      "[TD3] Episode  250 | Reward: -2.4285 | Final DLS:  120.86 nm\n",
      "[TD3] Episode  275 | Reward: -2.4285 | Final DLS:  120.86 nm\n",
      "[TD3] Episode  300 | Reward: -2.4285 | Final DLS:  120.86 nm\n",
      "Avg reward (last 50): -2.4285\n",
      "\n",
      "=== Trying config: TD3Config(actor_lr=0.0003, critic_lr=0.0003, gamma=0.99, tau=0.005, batch_size=64, policy_noise_frac=0.1, noise_clip_frac=0.2, explore_noise_frac=0.1, policy_delay=2, max_episodes=400, device='cpu', seed=1) ===\n",
      "[TD3] Episode   25 | Reward: -2.5191 | Final DLS:  120.86 nm\n",
      "[TD3] Episode   50 | Reward: -2.4285 | Final DLS:  120.86 nm\n",
      "[TD3] Episode   75 | Reward: -2.4285 | Final DLS:  120.86 nm\n",
      "[TD3] Episode  100 | Reward: -2.4285 | Final DLS:  120.86 nm\n",
      "[TD3] Episode  125 | Reward: -2.4285 | Final DLS:  120.86 nm\n",
      "[TD3] Episode  150 | Reward: -2.4285 | Final DLS:  120.86 nm\n",
      "[TD3] Episode  175 | Reward: -2.5680 | Final DLS:  120.86 nm\n",
      "[TD3] Episode  200 | Reward: -2.4285 | Final DLS:  120.86 nm\n",
      "[TD3] Episode  225 | Reward: -2.4285 | Final DLS:  120.86 nm\n",
      "[TD3] Episode  250 | Reward: -2.4285 | Final DLS:  120.86 nm\n",
      "[TD3] Episode  275 | Reward: -2.4285 | Final DLS:  120.86 nm\n",
      "[TD3] Episode  300 | Reward: -2.4285 | Final DLS:  120.86 nm\n",
      "Avg reward (last 50): -2.4404\n",
      "\n",
      "=== Trying config: TD3Config(actor_lr=0.0001, critic_lr=0.001, gamma=0.99, tau=0.005, batch_size=64, policy_noise_frac=0.15, noise_clip_frac=0.25, explore_noise_frac=0.08, policy_delay=2, max_episodes=400, device='cpu', seed=1) ===\n",
      "[TD3] Episode   25 | Reward: -2.4285 | Final DLS:  120.86 nm\n",
      "[TD3] Episode   50 | Reward: -2.4285 | Final DLS:  120.86 nm\n",
      "[TD3] Episode   75 | Reward: -2.4285 | Final DLS:  120.86 nm\n",
      "[TD3] Episode  100 | Reward: -2.4285 | Final DLS:  120.86 nm\n",
      "[TD3] Episode  125 | Reward: -2.4285 | Final DLS:  120.86 nm\n",
      "[TD3] Episode  150 | Reward: -2.4285 | Final DLS:  120.86 nm\n",
      "[TD3] Episode  175 | Reward: -2.4285 | Final DLS:  120.86 nm\n",
      "[TD3] Episode  200 | Reward: -2.4285 | Final DLS:  120.86 nm\n",
      "[TD3] Episode  225 | Reward: -2.4285 | Final DLS:  120.86 nm\n",
      "[TD3] Episode  250 | Reward: -2.4285 | Final DLS:  120.86 nm\n",
      "[TD3] Episode  275 | Reward: -2.4285 | Final DLS:  120.86 nm\n",
      "[TD3] Episode  300 | Reward: -2.4285 | Final DLS:  120.86 nm\n",
      "Avg reward (last 50): -2.4331\n",
      "\n",
      ">>> Best config selected: TD3Config(actor_lr=0.0001, critic_lr=0.001, gamma=0.99, tau=0.005, batch_size=64, policy_noise_frac=0.1, noise_clip_frac=0.2, explore_noise_frac=0.05, policy_delay=2, max_episodes=400, device='cpu', seed=0)\n",
      "\n",
      "You can now query the trained TD3 policy with any desired DLS in [83, 203] nm.\n",
      "Enter e.g. 150 (or press Enter to use 150 by default). Ctrl+C to exit.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Desired DLS (nm) [83..203]:  150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Suggested parameters (TD3 policy):\n",
      "  Time (min):        25.00  (bounds: 2.0..25.0)\n",
      "  Scanspeed (mm/s):  3000.00  (bounds: 3000.0..3500.0)\n",
      "  Fluence (J/cm²):   1.830  (bounds: 1.8300000429153442..1.909999966621399)\n",
      "Predicted DLS:       134.90 nm | Target: 150.00 nm | Error: 15.10 nm\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Desired DLS (nm) [83..203]:  95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Suggested parameters (TD3 policy):\n",
      "  Time (min):        25.00  (bounds: 2.0..25.0)\n",
      "  Scanspeed (mm/s):  3000.00  (bounds: 3000.0..3500.0)\n",
      "  Fluence (J/cm²):   1.830  (bounds: 1.8300000429153442..1.909999966621399)\n",
      "Predicted DLS:       134.90 nm | Target: 95.00 nm | Error: 39.90 nm\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Desired DLS (nm) [83..203]:  145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Suggested parameters (TD3 policy):\n",
      "  Time (min):        25.00  (bounds: 2.0..25.0)\n",
      "  Scanspeed (mm/s):  3000.00  (bounds: 3000.0..3500.0)\n",
      "  Fluence (J/cm²):   1.830  (bounds: 1.8300000429153442..1.909999966621399)\n",
      "Predicted DLS:       134.90 nm | Target: 145.00 nm | Error: 10.10 nm\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Desired DLS (nm) [83..203]:  190\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Suggested parameters (TD3 policy):\n",
      "  Time (min):        25.00  (bounds: 2.0..25.0)\n",
      "  Scanspeed (mm/s):  3000.00  (bounds: 3000.0..3500.0)\n",
      "  Fluence (J/cm²):   1.830  (bounds: 1.8300000429153442..1.909999966621399)\n",
      "Predicted DLS:       134.90 nm | Target: 190.00 nm | Error: 55.10 nm\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Desired DLS (nm) [83..203]:  150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Suggested parameters (TD3 policy):\n",
      "  Time (min):        25.00  (bounds: 2.0..25.0)\n",
      "  Scanspeed (mm/s):  3000.00  (bounds: 3000.0..3500.0)\n",
      "  Fluence (J/cm²):   1.830  (bounds: 1.8300000429153442..1.909999966621399)\n",
      "Predicted DLS:       134.90 nm | Target: 150.00 nm | Error: 15.10 nm\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "TD3 for Surrogate-Driven Parameter Optimization (PLAL)\n",
    "-----------------------------------------------------\n",
    "- Environment: Random Forest model that predicts DLS from [Time, ScanSpeed, Fluence].\n",
    "- Agent: TD3 (Twin Delayed DDPG) with:\n",
    "    * Twin critics (reduces Q overestimation bias)\n",
    "    * Target policy smoothing (robustness)\n",
    "    * Delayed policy/target updates (stability)\n",
    "- Goal: Given a target DLS (e.g., 150 nm), learn to propose parameters that hit it.\n",
    "- Action/State bounds:\n",
    "    Time(min):       [ 2.00, 25.00 ]\n",
    "    ScanSpeed(mm/s): [ 3000, 3500 ]\n",
    "    Fluence(J/cm²):  [ 1.83, 1.91 ]\n",
    "- DLS valid range (for reward scaling): [83, 203] nm\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import gym\n",
    "import numpy as np\n",
    "from gym import spaces\n",
    "import joblib\n",
    "from collections import deque\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple, Dict, Any, List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Reproducibility helpers\n",
    "# -------------------------------\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Constants (bounds + scaling)\n",
    "# -------------------------------\n",
    "ACTION_LOW  = np.array([ 2.0, 3000.0, 1.83], dtype=np.float32)\n",
    "ACTION_HIGH = np.array([25.0, 3500.0, 1.91], dtype=np.float32)\n",
    "\n",
    "DLS_MIN, DLS_MAX = 83.0, 203.0              # for reward normalization\n",
    "DLS_TOL = 1.0                                # success if |pred - target| < 1 nm\n",
    "EPISODE_STEPS = 10                           # environment horizon (concise exploration)\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Gym Environment: RF-as-Env\n",
    "# -------------------------------\n",
    "class DLS_Environment(gym.Env):\n",
    "    \"\"\"\n",
    "    A minimal Gym-compatible environment that wraps a trained Random Forest regressor.\n",
    "    State = current parameter vector [Time, ScanSpeed, Fluence]\n",
    "    Action = next proposed parameter vector (same shape, bounded)\n",
    "    Reward = - normalized absolute error to target DLS\n",
    "    Done   = success (error < 1 nm) or out of steps\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {\"render.modes\": []}\n",
    "\n",
    "    def __init__(self, model_path: str = \"random_forest_dls_model.pkl\"):\n",
    "        super().__init__()\n",
    "        # Load your trained Random Forest model\n",
    "        self.model = joblib.load(model_path)\n",
    "\n",
    "        # If you trained RF on scaled features, load/apply the SAME scaler(s) here:\n",
    "        # Example:\n",
    "        # self.scaler_X = joblib.load(\"feature_scaler.pkl\")  # optional\n",
    "        # self.scaler_y = joblib.load(\"target_scaler.pkl\")   # optional\n",
    "\n",
    "        self.action_space = spaces.Box(low=ACTION_LOW, high=ACTION_HIGH, dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(low=ACTION_LOW, high=ACTION_HIGH, dtype=np.float32)\n",
    "\n",
    "        self.target_dls = 150.0\n",
    "        self.current_step = 0\n",
    "        self.max_steps = EPISODE_STEPS\n",
    "        self.state = None\n",
    "\n",
    "    def _predict_dls(self, x: np.ndarray) -> float:\n",
    "        \"\"\"Predict DLS from RF; apply scaler if RF was trained with one.\"\"\"\n",
    "        X = x[None, :]  # shape (1,3)\n",
    "        # If you used a scaler during RF training, uncomment:\n",
    "        # X = self.scaler_X.transform(X)\n",
    "        pred = float(self.model.predict(X)[0])\n",
    "        # If target was scaled:\n",
    "        # pred = float(self.scaler_y.inverse_transform([[pred]])[0,0])\n",
    "        return pred\n",
    "\n",
    "    def reset(self, target_dls: float = None) -> np.ndarray:\n",
    "        \"\"\"Start a new episode with a random state (valid bounds).\"\"\"\n",
    "        self.current_step = 0\n",
    "        if target_dls is not None:\n",
    "            self.target_dls = float(target_dls)\n",
    "        self.state = self.action_space.sample()\n",
    "        return self.state.astype(np.float32)\n",
    "\n",
    "    def step(self, action: np.ndarray):\n",
    "        \"\"\"Apply action (next proposed parameters), get DLS prediction and reward.\"\"\"\n",
    "        self.current_step += 1\n",
    "        # Keep within physical bounds\n",
    "        self.state = np.clip(action, self.action_space.low, self.action_space.high).astype(np.float32)\n",
    "\n",
    "        # Predict DLS with surrogate model\n",
    "        predicted_dls = self._predict_dls(self.state)\n",
    "\n",
    "        # Dense reward: negative normalized absolute error\n",
    "        norm = (DLS_MAX - DLS_MIN)\n",
    "        reward = -abs(predicted_dls - self.target_dls) / norm\n",
    "\n",
    "        # Episode termination: success or horizon reached\n",
    "        done = (abs(predicted_dls - self.target_dls) < DLS_TOL) or (self.current_step >= self.max_steps)\n",
    "\n",
    "        info = {\"Predicted_DLS\": predicted_dls}\n",
    "        return self.state, float(reward), bool(done), info\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        pass\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Replay Buffer\n",
    "# -------------------------------\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity: int = 100000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, s, a, r, s2, d):\n",
    "        self.buffer.append((s, a, r, s2, d))\n",
    "\n",
    "    def sample(self, batch_size: int):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        s, a, r, s2, d = map(np.array, zip(*batch))\n",
    "        return s, a, r.reshape(-1, 1), s2, d.reshape(-1, 1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Neural Networks\n",
    "# -------------------------------\n",
    "class Actor(nn.Module):\n",
    "    \"\"\"\n",
    "    Actor maps state -> action. We use tanh() then scale to [low, high]\n",
    "    so the network naturally outputs bounded, physical actions.\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim: int, action_low: np.ndarray, action_high: np.ndarray, hidden: int = 256):\n",
    "        super().__init__()\n",
    "        self.action_low  = torch.tensor(action_low, dtype=torch.float32)\n",
    "        self.action_high = torch.tensor(action_high, dtype=torch.float32)\n",
    "        self.register_buffer(\"a_low\",  self.action_low)\n",
    "        self.register_buffer(\"a_high\", self.action_high)\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden), nn.ReLU(),\n",
    "            nn.Linear(hidden, hidden), nn.ReLU(),\n",
    "            nn.Linear(hidden, len(action_low))\n",
    "        )\n",
    "\n",
    "    def forward(self, s: torch.Tensor) -> torch.Tensor:\n",
    "        raw = self.net(s)\n",
    "        a = torch.tanh(raw)                      # (-1,1)\n",
    "        # scale to [low, high]\n",
    "        return 0.5 * (a + 1.0) * (self.a_high - self.a_low) + self.a_low\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    \"\"\"\n",
    "    Critic maps [state, action] -> Q-value. (One head per Q-network)\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim: int, action_dim: int, hidden: int = 256):\n",
    "        super().__init__()\n",
    "        self.q = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, hidden), nn.ReLU(),\n",
    "            nn.Linear(hidden, hidden), nn.ReLU(),\n",
    "            nn.Linear(hidden, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, s: torch.Tensor, a: torch.Tensor) -> torch.Tensor:\n",
    "        x = torch.cat([s, a], dim=-1)\n",
    "        return self.q(x)\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# TD3 Agent\n",
    "# -------------------------------\n",
    "@dataclass\n",
    "class TD3Config:\n",
    "    actor_lr: float = 1e-4\n",
    "    critic_lr: float = 1e-3\n",
    "    gamma: float = 0.99\n",
    "    tau: float = 0.005\n",
    "    batch_size: int = 64\n",
    "    policy_noise_frac: float = 0.10   # fraction of (high-low) for target smoothing\n",
    "    noise_clip_frac: float = 0.20     # clip target noise to this fraction\n",
    "    explore_noise_frac: float = 0.05  # exploration noise fraction of (high-low)\n",
    "    policy_delay: int = 2             # delayed actor/target updates\n",
    "    max_episodes: int = 400\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    seed: int = 0\n",
    "\n",
    "\n",
    "class TD3Agent:\n",
    "    def __init__(self, state_dim: int, action_low: np.ndarray, action_high: np.ndarray, cfg: TD3Config):\n",
    "        self.cfg = cfg\n",
    "        self.device = torch.device(cfg.device)\n",
    "\n",
    "        self.action_low  = torch.tensor(action_low,  dtype=torch.float32, device=self.device)\n",
    "        self.action_high = torch.tensor(action_high, dtype=torch.float32, device=self.device)\n",
    "        self.act_range   = (self.action_high - self.action_low)\n",
    "\n",
    "        # Networks\n",
    "        self.actor        = Actor(state_dim, action_low, action_high).to(self.device)\n",
    "        self.actor_target = Actor(state_dim, action_low, action_high).to(self.device)\n",
    "        self.critic1      = Critic(state_dim, len(action_low)).to(self.device)\n",
    "        self.critic2      = Critic(state_dim, len(action_low)).to(self.device)\n",
    "        self.critic1_t    = Critic(state_dim, len(action_low)).to(self.device)\n",
    "        self.critic2_t    = Critic(state_dim, len(action_low)).to(self.device)\n",
    "\n",
    "        # Target init\n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "        self.critic1_t.load_state_dict(self.critic1.state_dict())\n",
    "        self.critic2_t.load_state_dict(self.critic2.state_dict())\n",
    "\n",
    "        # Optims\n",
    "        self.actor_opt  = optim.Adam(self.actor.parameters(),  lr=cfg.actor_lr)\n",
    "        self.critic1_opt= optim.Adam(self.critic1.parameters(), lr=cfg.critic_lr)\n",
    "        self.critic2_opt= optim.Adam(self.critic2.parameters(), lr=cfg.critic_lr)\n",
    "\n",
    "        self.replay = ReplayBuffer(capacity=100_000)\n",
    "        self.total_it = 0  # gradient step counter\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def select_action(self, state: np.ndarray, explore: bool = True) -> np.ndarray:\n",
    "        \"\"\"Deterministic policy + optional exploration noise (bounded).\"\"\"\n",
    "        s = torch.tensor(state, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "        a = self.actor(s)  # already scaled to [low, high]\n",
    "        a = a.squeeze(0)\n",
    "\n",
    "        if explore:\n",
    "            # Gaussian exploration noise proportional to action range\n",
    "            noise_std = self.cfg.explore_noise_frac * self.act_range\n",
    "            noise = torch.normal(mean=torch.zeros_like(a), std=noise_std.to(self.device))\n",
    "            a = a + noise\n",
    "\n",
    "        # Clip to bounds (safety)\n",
    "        a = torch.max(torch.min(a, self.action_high), self.action_low)\n",
    "        return a.detach().cpu().numpy().astype(np.float32)\n",
    "\n",
    "    def train_step(self):\n",
    "        if len(self.replay) < self.cfg.batch_size:\n",
    "            return\n",
    "\n",
    "        self.total_it += 1\n",
    "\n",
    "        # Sample batch\n",
    "        s, a, r, s2, d = self.replay.sample(self.cfg.batch_size)\n",
    "        s  = torch.tensor(s,  dtype=torch.float32, device=self.device)\n",
    "        a  = torch.tensor(a,  dtype=torch.float32, device=self.device)\n",
    "        r  = torch.tensor(r,  dtype=torch.float32, device=self.device)\n",
    "        s2 = torch.tensor(s2, dtype=torch.float32, device=self.device)\n",
    "        d  = torch.tensor(d,  dtype=torch.float32, device=self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Target policy smoothing: actor_target(s2) + clipped noise\n",
    "            noise_std  = self.cfg.policy_noise_frac * self.act_range\n",
    "            noise_clip = self.cfg.noise_clip_frac  * self.act_range\n",
    "\n",
    "            target_a = self.actor_target(s2)\n",
    "            noise = torch.normal(mean=torch.zeros_like(target_a), std=noise_std.to(self.device))\n",
    "            noise = torch.clamp(noise, -noise_clip, noise_clip)\n",
    "\n",
    "            target_a = target_a + noise\n",
    "            target_a = torch.max(torch.min(target_a, self.action_high), self.action_low)\n",
    "\n",
    "            # Twin target critics and min for clipped double-Q\n",
    "            q1_t = self.critic1_t(s2, target_a)\n",
    "            q2_t = self.critic2_t(s2, target_a)\n",
    "            q_t_min = torch.minimum(q1_t, q2_t)\n",
    "\n",
    "            y = r + (1.0 - d) * self.cfg.gamma * q_t_min\n",
    "\n",
    "        # Critic updates (both critics)\n",
    "        q1 = self.critic1(s, a)\n",
    "        q2 = self.critic2(s, a)\n",
    "        critic1_loss = nn.MSELoss()(q1, y)\n",
    "        critic2_loss = nn.MSELoss()(q2, y)\n",
    "\n",
    "        self.critic1_opt.zero_grad()\n",
    "        critic1_loss.backward()\n",
    "        self.critic1_opt.step()\n",
    "\n",
    "        self.critic2_opt.zero_grad()\n",
    "        critic2_loss.backward()\n",
    "        self.critic2_opt.step()\n",
    "\n",
    "        # Delayed policy (actor) and target updates\n",
    "        if self.total_it % self.cfg.policy_delay == 0:\n",
    "            # Actor aims to maximize Q1(s, actor(s)) => minimize negative Q\n",
    "            actor_actions = self.actor(s)\n",
    "            actor_loss = - self.critic1(s, actor_actions).mean()\n",
    "            self.actor_opt.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            self.actor_opt.step()\n",
    "\n",
    "            # Soft target updates\n",
    "            with torch.no_grad():\n",
    "                tau = self.cfg.tau\n",
    "                for targ, src in zip(self.actor_target.parameters(), self.actor.parameters()):\n",
    "                    targ.data.mul_(1 - tau).add_(tau * src.data)\n",
    "                for targ, src in zip(self.critic1_t.parameters(), self.critic1.parameters()):\n",
    "                    targ.data.mul_(1 - tau).add_(tau * src.data)\n",
    "                for targ, src in zip(self.critic2_t.parameters(), self.critic2.parameters()):\n",
    "                    targ.data.mul_(1 - tau).add_(tau * src.data)\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Training utilities\n",
    "# -------------------------------\n",
    "def run_td3_training(env: DLS_Environment, agent: TD3Agent, episodes: int = 400, target_dls: float = 150.0,\n",
    "                     print_every: int = 10) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Train TD3 for a number of episodes. Returns training logs (rewards, last info).\n",
    "    \"\"\"\n",
    "    logs = {\"episode_rewards\": [], \"final_dls\": []}\n",
    "    for ep in range(1, episodes + 1):\n",
    "        state = env.reset(target_dls=target_dls)\n",
    "        ep_reward = 0.0\n",
    "        last_info = {}\n",
    "        for _ in range(env.max_steps):\n",
    "            action = agent.select_action(state, explore=True)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            agent.replay.push(state, action, reward, next_state, float(done))\n",
    "            agent.train_step()\n",
    "            state = next_state\n",
    "            ep_reward += reward\n",
    "            last_info = info\n",
    "            if done:\n",
    "                break\n",
    "        logs[\"episode_rewards\"].append(ep_reward)\n",
    "        logs[\"final_dls\"].append(last_info.get(\"Predicted_DLS\", np.nan))\n",
    "\n",
    "        if (ep % print_every) == 0:\n",
    "            print(f\"[TD3] Episode {ep:4d} | Reward: {ep_reward: .4f} | Final DLS: {last_info.get('Predicted_DLS', np.nan): .2f} nm\")\n",
    "    return logs\n",
    "\n",
    "\n",
    "def evaluate_agent(env: DLS_Environment, agent: TD3Agent, target_dls: float, trials: int = 20) -> Tuple[np.ndarray, float, float]:\n",
    "    \"\"\"\n",
    "    Deterministic evaluation: try multiple random restarts, pick the best action.\n",
    "    Returns (best_action, best_pred_dls, best_error_nm).\n",
    "    \"\"\"\n",
    "    best_action, best_pred, best_err = None, None, float(\"inf\")\n",
    "    for _ in range(trials):\n",
    "        state = env.reset(target_dls=target_dls)\n",
    "        action = agent.select_action(state, explore=False)  # no noise for inference\n",
    "        _, _, _, info = env.step(action)\n",
    "        pred = info[\"Predicted_DLS\"]\n",
    "        err = abs(pred - target_dls)\n",
    "        if err < best_err:\n",
    "            best_err = err\n",
    "            best_pred = pred\n",
    "            best_action = action.copy()\n",
    "    return best_action, float(best_pred), float(best_err)\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Simple hyperparameter tuning\n",
    "# -------------------------------\n",
    "def small_hparam_sweep(env_path: str = \"random_forest_dls_model.pkl\",\n",
    "                       seeds: List[int] = [0, 1],\n",
    "                       configs: List[TD3Config] = None,\n",
    "                       episodes: int = 300,\n",
    "                       target_dls: float = 150.0) -> Tuple[TD3Agent, TD3Config, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Very small TD3 sweep: tests a handful of configs & seeds to pick the best by average reward.\n",
    "    Extend this list if you have more compute time.\n",
    "    \"\"\"\n",
    "    if configs is None:\n",
    "        configs = [\n",
    "            TD3Config(actor_lr=1e-4, critic_lr=1e-3, explore_noise_frac=0.05, policy_noise_frac=0.10, noise_clip_frac=0.20, policy_delay=2),\n",
    "            TD3Config(actor_lr=3e-4, critic_lr=3e-4, explore_noise_frac=0.10, policy_noise_frac=0.10, noise_clip_frac=0.20, policy_delay=2),\n",
    "            TD3Config(actor_lr=1e-4, critic_lr=1e-3, explore_noise_frac=0.08, policy_noise_frac=0.15, noise_clip_frac=0.25, policy_delay=2),\n",
    "        ]\n",
    "\n",
    "    best_score = -1e9\n",
    "    best_agent, best_cfg, best_logs = None, None, None\n",
    "\n",
    "    for seed in seeds:\n",
    "        for cfg in configs:\n",
    "            cfg = TD3Config(**{**cfg.__dict__, \"seed\": seed})\n",
    "            print(f\"\\n=== Trying config: {cfg} ===\")\n",
    "            set_seed(cfg.seed)\n",
    "\n",
    "            env = DLS_Environment(model_path=env_path)\n",
    "            agent = TD3Agent(state_dim=3, action_low=ACTION_LOW, action_high=ACTION_HIGH, cfg=cfg)\n",
    "            logs = run_td3_training(env, agent, episodes=min(episodes, cfg.max_episodes), target_dls=target_dls, print_every=25)\n",
    "\n",
    "            avg_last50 = float(np.nanmean(logs[\"episode_rewards\"][-50:])) if len(logs[\"episode_rewards\"]) >= 50 else float(np.nanmean(logs[\"episode_rewards\"]))\n",
    "            print(f\"Avg reward (last 50): {avg_last50:.4f}\")\n",
    "\n",
    "            if avg_last50 > best_score:\n",
    "                best_score, best_agent, best_cfg, best_logs = avg_last50, agent, cfg, logs\n",
    "\n",
    "    print(\"\\n>>> Best config selected:\", best_cfg)\n",
    "    return best_agent, best_cfg, best_logs\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Main: train + interactive query\n",
    "# -------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) (Optional) quick hyperparameter sweep to pick a good TD3 config\n",
    "    best_agent, best_cfg, _ = small_hparam_sweep(\n",
    "        env_path=\"random_forest_dls_model.pkl\",\n",
    "        seeds=[0, 1],\n",
    "        episodes=300,           # increase if you want stronger policies\n",
    "        target_dls=150.0\n",
    "    )\n",
    "\n",
    "    # 2) Let the user query any target DLS and get suggested parameters\n",
    "    print(\"\\nYou can now query the trained TD3 policy with any desired DLS in [83, 203] nm.\")\n",
    "    print(\"Enter e.g. 150 (or press Enter to use 150 by default). Ctrl+C to exit.\")\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            s = input(\"\\nDesired DLS (nm) [83..203]: \").strip()\n",
    "            target = 150.0 if (s == \"\" or s is None) else float(s)\n",
    "            target = float(np.clip(target, DLS_MIN, DLS_MAX))\n",
    "\n",
    "            env = DLS_Environment(model_path=\"random_forest_dls_model.pkl\")\n",
    "            action, pred_dls, err = evaluate_agent(env, best_agent, target_dls=target, trials=30)\n",
    "\n",
    "            print(\"\\nSuggested parameters (TD3 policy):\")\n",
    "            print(f\"  Time (min):        {action[0]:.2f}  (bounds: {ACTION_LOW[0]}..{ACTION_HIGH[0]})\")\n",
    "            print(f\"  Scanspeed (mm/s):  {action[1]:.2f}  (bounds: {ACTION_LOW[1]}..{ACTION_HIGH[1]})\")\n",
    "            print(f\"  Fluence (J/cm²):   {action[2]:.3f}  (bounds: {ACTION_LOW[2]}..{ACTION_HIGH[2]})\")\n",
    "            print(f\"Predicted DLS:       {pred_dls:.2f} nm | Target: {target:.2f} nm | Error: {err:.2f} nm\")\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nExiting. Goodbye!\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Input error ({e}). Please enter a number between {DLS_MIN} and {DLS_MAX}.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8faf1a-ec3f-4206-be1a-7829d413c7b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a764edf-0889-41a4-a64b-72c9e5f3369f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
